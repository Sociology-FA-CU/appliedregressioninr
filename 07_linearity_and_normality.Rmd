---
title: "Linearity and normality"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
bibliography: references_lecture_07.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r data-and-packages}
library(tidyverse)
library(splines)
library(lspline)
library(ggeffects)

vote = read.csv("data/parl_vote_2017.csv")
un = read.table("data/UnitedNations.txt")
```

In this chapter, we will look into ways of modeling nonlinear relationship, as well solving the problem of nonconstant variance.

# Modeling nonlinearity

There are many ways one could go around modeling nonlinear relationship through regression models.
We will present three basic, which are common and easy to implement: a) categorization b ) polynomial functions and c) splines, also called piecewise polynomials.

For demonstration, we will use data from the eight round of the European Social Survey.
More specificaly, data on voter turnouver per age group in the last pairlamentary elections in the Czech republic.
Just by ploting the data, we can notice a nonlinear relationship between voter turnout and age:

```{r age-vs-vote, fig.cap="Age vs voter turnout"}
ggplot(data = vote, aes(x = agea, y = vote)) +
  geom_point() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(x = "Age", y = "% of respondents who attended the elections")
```

We can reach the same conclusion by fitting a linear model and checking the residual plot:

```{r linear-mod-example, include=TRUE}
mod_linear = lm(vote ~ agea, data = vote)

plot(mod_linear, which = 1)
```

While the sample size is relatively small, the residual plot suggests that simple linear regression may not give us the full picture on the relationship between age and voter turnout.

## Categorization

THe easiest and by far the most popular way to model nonlinear relationship between variables is to simply categorize the independent variable.
The categorization can be done in multiple ways and choice is mostly arbitrary.
The `ggplot2` is particulary helpful, as it offers three categorization function: `cut_interval()`, `cut_number()` and `cut_width()`.

The function `cut_interval()` discretize a metric variable into `n` group of equal range.
For example, to categorize age into three groups with equal range and plot the model:

```{r cut-interval-example, include=TRUE}
vote$agea_interval = cut_interval(vote$agea, n = 3)

mod_interval = lm(vote ~ agea_interval, data = vote)

plot(ggeffect(mod_interval))
```

As we can see, the age variable was cut into three groups, each with range of 24.65.
Alternatively we can use the `cut_number()` function to categorize age into `n` groups with (aproximately) the same number of observations:

```{r cut-number-example, include=TRUE}
vote$agea_number = cut_number(vote$agea, n = 3)

mod_number = lm(vote ~ agea_number, data = vote)

plot(ggeffect(mod_number))
```

The third option is to discretise the metric variable into categories of equal `width`.
This can be done using the `cut_width()` function:

```{r cut-width-example, include=TRUE}
vote$agea_width = cut_width(vote$agea, width  = 30)

mod_width = lm(vote ~ agea_width, data = vote)

plot(ggeffect(mod_width))
```

The last option is to base the categorization of our data not on the properties of our sample, such as range or quantiles, but determine our the categories according to a theory.
For example, in the context of political behavior, people are often categorized into three cohort: a) age 25 and younger b) 26 to 55 years and c) 56 or more.

```{r cut-theory-example, include=TRUE}
vote$agea_theory = case_when(
                             vote$agea <= 25 ~ "25 or less",
                             vote$agea <= 55 ~ "26 to 55",
                             vote$agea > 55 ~ "55 or more")

mod_theory = lm(vote ~ agea_theory, data = vote)

plot(ggeffect(mod_theory))
```

While modeling nonlinear relationships using categorization of metric variables is conveniently easy, it also suffer from a number of problems.
@harrellRegressionModelingStrategies2001 provides a list of major problems, among others:

-   Estimated values will have reduced precision, and associated tests will have reduced power

-   Categorization assumes that the relationship between the predictor and the response is flat within intervals

-   Categorization assumes that there is a discontinuity in response as interval boundaries are crossed.

-   Cutpoints are arbitrary and manipulatable; cutpoints can be found that can result\
    in both positive and negative associations

For these and more reasons described by Harrel, the categorization of interval variables should be in most cases avoided.

## Polynomials

Another option for modeling nonlinearity is to use polynomial functions.
Polynomials functions are ones which include an exponent of a variable.
An example of a regression model with polynomial function would be

$$
vote = \beta_0 + \beta_1*age + \beta_2*age^2
$$

with $age$ being taken to the power of two.
This effectively replaces the assumption that the relation between vote and age is a straight line with the assumption that the relationship has a shape of a parabola.
We could also use higher order exponents to model increasingly flexible relationships.

There are two types of polynomials - raw polynomials and orthogonal ones.
The raw polynomial is simple the variable taken to the power of *k*.
To use second order raw polynomial in a regression model we can write:

```{r raw-poly-example1, eval=FALSE}
lm(vote ~ poly(agea, 2, raw = TRUE), data = vote)
```

or alternatively:

```{r raw-poly-example2, eval=FALSE}
lm(vote ~ agea + I(agea^2), data = vote)
```

Note that both give the same result.

On the other hand, orthogonal polynomials are computed in such a way that the variable $x^k$ is uncorrelated, i.e. orthogonal, to the original variable $x$.
To compute a second order orthogonal polynomial:

```{r orth-poly-example, eval=TRUE}
lm(lm(vote ~ poly(agea, 2), data = vote))
```

Which of these two types of polynomials should we choose?
Both types of polynomials lead to the same model in the terms of fit and predictions.
The advantage of raw polynomials is that the regression coefficients have the traditional interpretation of *change in* $y$, when $x$ is changed by one and $x^k$ is fixed at zero.
However, the interpretation of variables transformed in such way is difficult no matter what.
The major advantage of the orthogonal polynomials is that we can compute the proportion variance explained by each variable and therefore assess its contribution to the predictive power.
If our goal is to interpret the relationship between variables visually through the marginal effects plots, or if only include the variable as a control, the choice of polynomial type does not matter.

Since we nonlinear relationship are best interpreted visually, we prefer orthogonal polynomials.
We can visualize a polynomial regression model using the `ggeffects` package:

```{r poly-example}
mod_poly = lm(vote ~ poly(agea, 2), data = vote)

plot(ggeffect(mod_poly))
```

While polynomials are often a better option than the categorization approach, they are not without problems.
The main problem is that polynomial functions are non local [@fox2015: @harrellRegressionModelingStrategies2001], which means that data in one region can strongly influence predicted values in other regions, especially those with small number of observations.
This means that the polynomial regression curves often get "wiggly" and overly curved at the ends.
Therefore, polynomial regression tend to provide a good fit only in the regions with most data and are often bad at extrapolation.
Polynomials are also bad at modeling relationships that are nonlinear only in certain regions and linear in other.
For example, a second order polynomial leads by definition to the shape of a parabola and the regression has to predict a curve along the entire range of data.
This means that polynomials are bad at modeling relationship that tend to level of at same point.
These problems are especially pronounced with higher degrees polynomials.

In the graph below, we can see, how polynomials can be particularly bad at modeling relationships, that are nonlinear in some regions and linear in others.
The blue line is for second order polynomial, the blue one for third order one.

```{r poly-wiggle, fig.cap="Problem with fitting polynomials to asymptotic data", warning=FALSE, message=FALSE, echo=FALSE}
ggplot(un, mapping = aes(x = GDPperCapita, y = infantMortality)) +
  geom_point() +
  geom_smooth(method = "lm", se = F, formula = y ~ poly(x,2)) +
  geom_smooth(method = "lm", se = F, formula = y ~ poly(x,3), color = "tomato") +
  labs(caption = "Blue line = 2nd order polynomial model, red = 3rd order polynomial model")
```
