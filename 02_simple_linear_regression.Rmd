---
title: "Simple linear regression"
output: html_document
csl: apa.csl
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

```

```{r data-and-packages, echo=FALSE}
library(tidyverse)
library(patchwork)
library(here)


countries <- read.csv(here("data", "countries.csv"))
```

There are two goals for this lecture. First, we will do a quick recap of some basic concepts from statistics. Second, we will introduce basic building blocks for regression analysis (kind of regression blue print or basic plan) and we will try building and interpreting simple linear regression in R.

# 1 Quick recap on selected concepts in statistics

You should already be familiar with the concepts introduced in this section (remember that the course prerequisities are Statistics I and Statistics II), so at this place, we will be very brief about them.

## Variable, its distribution, variability and mean

A variable is an attribute, which can take different value. E.g. height is a variable as different people can have different height. So is an opinion of something.

For a variable with known values, we can construct its distribution. Probability distribution gives probabilities of occurrence of different possible values. We can also show distribution using frequencies, i.e., actual counts, rather than probability distribution.

Below, there are some commonly used plot types which help understand distribution of a variable visually.

```{r}

x = rnorm(100) - 1 
names(x) = rep("x", 100)
y = rnorm(100) + 1
names(y) = rep("y", 100)
z = c(x,y)

data = data.frame(origin=names(z), value=z, row.names=NULL)


p1 <- data %>% ggplot(aes(x=z))+geom_histogram(bins = 20, color="white") + labs(title = "Histogram")
p2 <- data %>% ggplot(aes(x=z))+geom_density(fill="grey") + labs(title = "Density plot")
p3 <- data %>% ggplot(aes(y=z, x=""))+geom_boxplot() + labs(x = "") + labs(title = "Box plot")
p4 <- data %>% ggplot(aes(y=z, x=""))+geom_violin()+geom_boxplot(width=0.4) + labs(x = "") + labs(title = "Violin plot")

(p1 | p2 ) / (p3 | p4 )

```

Variability within a variable can either be expressed visually as distribution (plots above), or it can be expressed with a numeric value, typically variance.

Variance of the variable z is the expected value of the squared deviation from its mean, we use the following formula:

$$
var(z) = E[(z_i-\mu_z)^2] = \frac{\sum (z_i - \bar{z})}{n}
$$

We typically work with the standard deviation (transformed variance) rather than the variance because standard deviation is on the original scale of the variable distribution. The formula is as follows:

$$
\sigma_z = \sqrt{var{(z)}}
$$

We also use point estimates to simplify a variable. A typical example is arithmetic mean.

We refer to the arithmetic mean of variable z as E(z) generally, or Î¼~z~ to refer specifically to population mean or $\overline{x}$ to refer to sample mean.

## Sampling distribution

Imagine you collect a survey sample from population. It is only one of many and many theoretical samples you could have collected (e.g, there are many ways you can sample 1000 people from the Czech population). It follows that when you use the sample to calculate some values from the variables in the sample (mean, variance, ... regression coefficient), these are not the only possible values. In fact, each of these values is just one data point from a theoretical distribution of all the different estimates you could calculate from all the possible samples. This theoretical distribution is called sampling distribution.

> "The sampling distribution is the set of possible datasets that could have been observed if the data collection process had been re-done, ..." [@gelman2020, pp.50]

Standard deviation of the sampling distribution (i.e., of the distribution of a given estimate) is called standard error and it equals $\frac{\sigma}{\sqrt n}$.

Confidence intervals are extension of standard errors. If the distribution is normal, 95 % confidence intervals are constructed by subtracting and adding 2 standard errors to the point estimate, see picture below (beta-hat is the estimate of the regression coefficient beta from the data).

```{r out.width = "50%", echo=FALSE, fig.align="center"}

knitr::include_graphics(here("images","sampling_distribution.PNG"))
```

<center>

<font size="1">Source: [@gelman2020, pp. 51]</font>

</center>

Note that the sampling distribution is not technically normal. It is Student's t-distribution (or just t-distribution) which will only converge to normal when the number ob observations is large enough, ca 30 and more.

## Statistical significance and hypothesis testing

Conventional wisdom: statistical significance is p-value less than 0.05, relative to some null hypothesis (hypothesis of no difference / no effect). Fair enough, but remember that the 0.05 value is arbitrary.

> "[p-value is] the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value" [@wasserstein2016]

Intuition behind statistical significance: an estimate is said to be NOT statistically significant if the observed value could reasonably be explained by chance.

This thinking arises from a still dominant yet heavily criticized approach of so called null-hypothesis testing (NHT): - null hypothesis: estimate = 0 - alternative hypothesis: estimate != 0

## Critique of NHT

-   NH is unrealistic in social sciences (only a matter of sample size, with enough data, everything is statistically significant)
-   NH is theoretically uninteresting (effect size and variations of effect sizes in different groups is what really matters)
-   NH is a very low threshold for any analysis, because non-rejection tells us that there is not even enough information in the data to move beyond the banal null hypothesis of no difference.
-   Even statistically non-significant data can carry important information (not just for meta-analysis).

## Critique of malpractice in using p-value

-   p-value is often used as a license for making a claim of a scientific finding (or implied truth) while neglecting many other important considerations ("design of a study, the quality of the measurements, the external evidence for the phenomenon under study, and the validity of assumptions that underlie the data analysis." [@wasserstein2016]
-   p-value is often used incorrectly (significance chasing a.k.a. p-hacking): (1) multiple statistical tests, (2) choice of data to be presented based on statistical-significance result.
-   p-value is often interpreted incorrectly (such as when non-significant p-value is considered evidence for no difference).
-   There is (usually almost) no difference between 5.1% and 4.9% significance level.

## Examining relationship between variables

In social sciences, we often want to know how two variables are associated, i.e., how they vary together (co-vary).

A basic measure of association is called covariance. Many measures of association draw on it one way or another. It is very closely related to variance. Inspect the the formulas below to see for yourself:

$$
var(x) = E[(x_i - \mu_x)^2] = E[(x_i - \mu_x)*(x_i - \mu_x)] = \\ \frac{\sum[(x_i - \mu_x)*(x_z-\mu_x)]}{n}
$$ 

<br>

$$
cov(x) =E[(x_i - \mu_x)*(y_i - \mu_y)] = \\ \frac{\sum[(x_i - \mu_x)*(y_i-\mu_y)]}{n}
$$

The value of covariance is rarely useful as the end product. Just like we prefer standard deviation as a scaled form of variance, we prefer correlation as scaled form of covariance. While standard deviation is scaled to the scale of the original variable, correlation is scaled to take a value between -1 and 1. In this sense, correlation is standardized covariance and measures strength of association.

If you square Pearson correlation coefficient between two variables, you get the proportion of variance in one variable explained by knowledge of the value for the other variable. This relationship is symmetrical (if x explains 20% of variation in y, then y explains 20% of variation in x). When you think about it, this means that we should not understand correlation coefficient as linear in the sense that increase of correlation coefficient by 0.1 always represent the same increase in the strength of association. If the coefficient is 0.2, we explain only 0.04, i.e., 4% of variance in x by knowing the value of y. If the coefficient is 0.3, it is 9%. If the coefficient is 0.4, it is 16%. So moving from 0.2 to 0.3 means 5 percentage point decrease of unexplained variance, whereas moving from 0.3 to 0.4 means 7 percentage point decrease of unexplained variance.

Visual representation of different values of Pearson correlation coefficient puts things into perspective:

```{r message=FALSE}
complement <- function(y, rho, x) {
  if (missing(x)) x <- rnorm(length(y)) # Optional: supply a default if `x` is not given
  y.perp <- residuals(lm(x ~ y))
  rho * sd(y.perp) * y + y.perp * sd(y) * sqrt(1 - rho^2)
}

y <- rnorm(50, sd=10)
x <- 1:50 # Optional
rho <- c(0, 0.2, 0.4, 0.6, 0.8, 1)
X <- data.frame(z=as.vector(sapply(rho, function(rho) complement(y, rho, x))),
                rho=ordered(rep(signif(rho, 2), each=length(y))),
                y=rep(y, length(rho)))
    

ggplot(X, aes(y,z, group=rho)) + 
  geom_smooth(method="lm", se=FALSE) + 
  geom_rug(sides="b") + 
  geom_point(aes(fill=rho), alpha=1/2, shape=21) +
  facet_wrap(~ rho)
```

<font size="1"> Code by whuber from: <https://stats.stackexchange.com/questions/15011/generate-a-random-variable-with-a-defined-correlation-to-an-existing-variables></font>

Correlation coefficient and regression coefficient are conceptually a different thing. While correlation coefficient shows the strength of association, regression coefficient shows how, on average, the value of y changes when the value of x changes by one unit. The fact that the regression lines in the set of plots above get steeper and steeper as the correlation gets stronger results from the data generation process. But it is no necessity. There can be steeper regression lines between less correlated variables and vice versa. See another set of simulated data below which demonstrate it.

```{r message=FALSE}
set.seed(42)
edu <- round (rnorm (100, mean = 15, sd = 4), digits=0)
set.seed (24)
rand <- round(rnorm (100, mean = 1000, sd = 50), digits=0)
rand.2 <- round(rnorm (100, mean = 2000, sd = 300), digits=0)
inc <- edu*rand
inc.2 <- edu*rand.2

data <- data.frame(x = edu, y = c(inc, inc.2), group = c(rep(1,100), rep(2,100)), row.names = NULL)

data %>% ggplot(aes(x=x, y=y, color = group, group=group))+
  geom_point()+
  geom_smooth(method = "lm", se=FALSE)+
  labs(title = "Simulated data",
       subtitle = "Light blue population has higher regression coefficient but lower correlation",
       x = "Years of education",
       y = "Monthly income in CZK")+
  theme(legend.position = "none")

```

Finally, the four plots below show that the message sent out by visualization can differ based on the tools we use. The plots below all visualize association between the same two continuous variables. Plotting just the regression line or just the points can send out a fairly different message about the association. Combination of the two seems more appropriate in this particular situation. Also notice that using straight line is not the only way to model association between two variables.

```{r warning=FALSE, message=FALSE}

p1 <- countries %>% ggplot(aes(x=uni_prc, y=life_exp))+
  geom_point()+
  scale_y_continuous (limits = c(76,83))+
  labs(x="",
       y="life expectancy")

p2 <- countries %>% ggplot(aes(x=uni_prc, y=life_exp))+
  geom_smooth(method = "lm", se=FALSE)+
    scale_y_continuous (limits = c(76,83))+
  labs(x="",
       y="")

p3 <- countries %>% ggplot(aes(x=uni_prc, y=life_exp))+
  geom_point()+
  geom_smooth(method = "lm", se=FALSE)+
    scale_y_continuous (limits = c(76,83))+
  labs(x="percentage of people with university degree",
       y="life expectancy")

p4 <- countries %>% ggplot(aes(x=uni_prc, y=life_exp))+
  geom_point()+
  geom_smooth()+
    scale_y_continuous (limits = c(76,83))+
  labs(x="percentage of people with university degree",
       y="")


patchwork <- (p1 | p2 ) / (p3 | p4 )

patchwork +   plot_annotation(title = 'Association between university education and life expectancy in selected countries')

```

# 2 Linear regression: basic building blocks

The simple (i.e., bivariate) regression line has the following formula:

$$
y = \alpha + \beta*x
$$

It can also be generalized as follows:

$$
y = \beta_0 + \beta_1*x
$$

$\alpha$ or $\beta_0$ in the generalized version is the intercept, i.e. the value of Y when X = 0. 

$\beta$ or $\beta_1$ in the generalized version is the slope, i.e. rate of change in Y when X changes by 1 unit.

As on the figure below, the actual observations usually do not fall exactly on the line. The difference between the blue regression line and each actual observation is called the error term or residual (more about the difference between the two below) and is shown as red line the figure.

```{r out.width = "50%", echo=FALSE, fig.align="center"}

knitr::include_graphics(here("images","linear-regression.png"))
```

<center>

<font size="1">Image source: <http://www.sthda.com/english/articles/40-regression-analysis/167-simple-linear-regression-in-r/></font>

</center>

Hence the full simple regression model has the following formula where epsilon represents the error term for the *i-th* observation.

$$
y = \alpha + \beta*x + \epsilon_i
$$

We use "error term" to refer to the (theoretical) difference between the real value of Y and the real average value of Y for given X. We use "residual" for the difference between our observed value of Y (can be distorted by measurement error) and our regression model (can be wrong for a variety of reasons). This means that error term is a theoretical concept while the residual is the value available in our data when we do statistical modeling.

In other words, errors are the deviations of the theoretical (i.e. measured without measurement error) observations from the real population conditional mean. Residuals are the deviations of the actual observations we have from the sample conditional mean. Residuals are observable estimates of the unobservable random errors.

As this distinction is theoretical, we often see the terms used interchangeably in practice.

## Ordinary least square

The algorithm used to estimate the regression line works so as to minimize the sum of squares of residuals (Residual Sum of Squares, RSS). Hence linear regression is sometimes also referred to as ordinary least square regression or OLS. For details on how OLS is calculated, see Fox [-@fox2015, pp. 83].

```{r, warning=F, message=FALSE, fig.width=5}
set.seed(1234)

ss_df = tibble::tibble(x = 1:10,
                       y = 2 + x + rnorm(n = 10, sd = 2))

ss_mod = lm(y ~ x, data = ss_df)
ss_df$pred = predict(ss_mod)
ss_df$label_pos = ifelse(ss_df$pred < ss_df$y,
                         ss_df$pred + (ss_df$y - ss_df$pred) / 2,
                         ss_df$pred - (ss_df$pred - ss_df$y) / 2)
ss_df$res = ss_df$y - ss_df$pred

ggplot(ss_df,aes(x = x, y = y, label = round(y - pred, 1))) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  geom_segment(aes(xend = x, yend = pred, color = "tomato"), show.legend = F) +
  geom_text(aes(x = x, y = label_pos), nudge_x = 0.2, color = "tomato")
  
```

$$
\small Sum\:of\:squares=-2^2+1.1^2+2.8^2+(-4)^2+1.6^2+1.8^2+(-0.3^2)+(-0.2^2)+(-0.1^2)+(-0.7^2)=35.3
$$
$$
\small Sum\:of\:residuals=-2+1.1+2.8-4+1.6+1.8-0.3-0.2-0.1-0.7=0
$$

## Regression and t-test

The line in the linear regression model can be perceived as conditional mean (as in the picture below). That is reminiscent of t-test. In fact, simple regression with one binary predictor is equivalent to t-test. The regression coefficient of one binary predictor coded as 0 and 1 represents the difference of means of the two groups.


```{r message=FALSE, warning=FALSE}


countries %>% mutate(postsoviet = as.numeric(postsoviet=="yes")) %>% 
  ggplot(aes(x=postsoviet, y = life_exp))+
     stat_summary(fun = mean, geom = "point", shape=20, size=8, color="red", fill="red") + 
    stat_summary(fun.data = mean_se, geom = "errorbar", color = "red", width=0.1, fun.args = list(mult = 2))+
    geom_point(alpha=0.3)+ 
  geom_smooth(method = "lm", se=FALSE)+
  scale_x_continuous(breaks = c(0,1), labels = c("non-postsoviet", "postsoviet"))+
  labs(x = "",
       y = "life expectancy",
       caption = "Red error bars indicate 95% CI of the mean values")
```

## Centering predictors for better interpretation

There are two commonly used types of centering. Centering by subtracting the mean results in intercept interpreted as the values of y when predictor x is set to its mean. Alternatively, we can use conventional centering point such such as 100 for IQ. Then the intercept is interpreted as value of y when the IQ is 100. 

The only reason for centering in OLS is for interpretation purposes. 


# References
