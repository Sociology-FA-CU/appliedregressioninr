---
title: "Simple linear regression"
subtitle: "Jaromír Mazák & Aleš Vomáčka"
author: "Dpt. of Sociology, Faculty of Arts, Charles University"
date: "Last updated in February 2022"
output:
  ioslides_presentation:
    widescreen: true
    incremental: true
csl: "../apa.csl"
bibliography: references_lecture_02.bib
editor_options: 
  chunk_output_type: console
zotero: "ksoc-ff-uk-quantitative"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(here)
library(polynom)
library(patchwork)

countries <- read.csv(here("data", "countries.csv"))
```

## Goals for today

-   Regression as a tool: Basic concepts and building blocks for regression analysis
-   Centering variables for better interpretation
-   Dummy variables for inclusion of categorical predictors
-   Building and interpreting simple linear regression in R (yay, we are gonna work in R!)

# Linear regression: basic building blocks

## The line

The simple (i.e., bivariate) regression line has the following formula:

$$
y = \alpha + \beta*x
$$

-   where $\alpha$ is the intercept, and $\beta$ is the slope. (What does it mean?)


## The line and the points

<div class="columns-2">

-   The formula $y = \alpha + \beta*x$  represents the line.
-   But the actual value of Y is usually below or above the line.
-   The actual value of Y = linear model + random error term
-   $y = \alpha + \beta*x + \epsilon_i$

<div>

![Regression plan](../images/linear-regression.png) <font size="1">Source: <http://www.sthda.com/english/articles/40-regression-analysis/167-simple-linear-regression-in-r/></font>

</div>

## Error vs. Residual

-   Error (theoretical) = difference between the real value of Y and the real average value of Y for given X
-   Residual (computational) = difference between our observed value of Y and its expected value (expected by our model)

## Ordinary least square (OLS)

<div class="columns-2">

-   Another name for linear regression
-   The model is estimated by minimizing the sum of squares of residuals. (By definition, the sum of residuals is then 0.)


-   $\small Residual\:sum\:of\:squares=-2^2+1.1^2+\\\small2.8^2+(-4)^2+1.6^2+1.8^2+\\\small(-0.3^2)+(-0.2^2)+\\\small(-0.1^2)+(-0.7^2)=35.3$

-   $\small Sum\:of\:residuals=-2+1.1\\\small+2.8-4+1.6+1.8-0.3\\\small-0.2-0.1-0.7=0$

<div>

```{r, warning=F, message=FALSE, fig.width=5}
set.seed(1234)

ss_df = tibble::tibble(x = 1:10,
                       y = 2 + x + rnorm(n = 10, sd = 2))

ss_mod = lm(y ~ x, data = ss_df)
ss_df$pred = predict(ss_mod)
ss_df$label_pos = ifelse(ss_df$pred < ss_df$y,
                         ss_df$pred + (ss_df$y - ss_df$pred) / 2,
                         ss_df$pred - (ss_df$pred - ss_df$y) / 2)
ss_df$res = ss_df$y - ss_df$pred

ggplot(ss_df,aes(x = x, y = y, label = round(y - pred, 1))) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  geom_segment(aes(xend = x, yend = pred, color = "tomato"), show.legend = F) +
  geom_text(aes(x = x, y = label_pos), nudge_x = 0.2, color = "tomato")
  
```

<font size="3">For details on how OLS is calculated, see @fox2015 [pp. 83]</font>

<div>

## Regression and t-test

The line in the model can be thought of as conditional mean (as in the picture below). Simple regression with one binary predictor is equivalent to t-test.

```{r message=FALSE, warning=FALSE}


countries %>% mutate(postsoviet = as.numeric(postsoviet=="yes")) %>% 
  ggplot(aes(x=postsoviet, y = life_exp))+
     stat_summary(fun = mean, geom = "point", shape=20, size=8, color="red", fill="red") + 
    stat_summary(fun.data = mean_se, geom = "errorbar", color = "red", width=0.1, fun.args = list(mult = 2))+
    geom_point(alpha=0.3)+ 
  geom_smooth(method = "lm", se=FALSE)+
  scale_x_continuous(breaks = c(0,1), labels = c("non-postsoviet", "postsoviet"))+
  labs(x = "",
       y = "life expectancy",
       caption = "Red error bars indicate 95% CI of the mean values")
```

## Centering predictors for better interpretation

-   Centering in OLS - interpretation purposes
-   Centering predictors by subtracting the mean: intercept interpreted as value of Y when the value of X is set to its mean
-   Using conventional centering point (such as subtracting 100 in IQ)

## Dummy variables

Categorical variables among predictors are usually treated as dummies:

-   Binary factors: transformed to one dummy variable
-   Multivariate factors: transformed into set of binary dummy variables (n-1)
-   Regression coefficients identify differences in group means compared to one reference group

## Sets of binary dummy variables

```{r}
set.seed(41)
dummies <- countries %>% select(di_cat) %>% sample_n(5)
dummies <- fastDummies::dummy_cols(dummies)
knitr::kable(dummies)
```

-   Decide for reference category and do not include it in the model
-   The mean for the reference category will be captured by the intercept
-   The coefficients of the dummies = distances of their conditional mean from the reference category

# Let's see this in R


## References



