<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Quick recap on selected concepts in statistics</title>

<script src="site_libs/header-attrs-2.9/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>




<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Applied Regression in R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lecture Notes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="00_quick_recap.html">Quick recap on selected concepts in statistics</a>
    </li>
    <li>
      <a href="01_intro.html">Introduction - goals of regression analysis</a>
    </li>
    <li>
      <a href="02_simple_linear_regression.html">Simple linear regression</a>
    </li>
    <li>
      <a href="03_multiple_linear_regression.html">Multiple linear regression</a>
    </li>
    <li>
      <a href="04_model_visualization.html">Ploting regression models, marginal effects</a>
    </li>
    <li>
      <a href="05_model_fit.html">Model fit</a>
    </li>
    <li>
      <a href="06_assumptions.html">Assumptions of linear models</a>
    </li>
    <li>
      <a href="06_diagnostics.html">Regression diagnostics</a>
    </li>
    <li>
      <a href="07_linearity_and_normality.html">Linearity</a>
    </li>
    <li>
      <a href="08_heterscedasticity.html">Homoscedasticity</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Slides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="01_slides_intro.html">Introduction - goals of regression analysis</a>
    </li>
    <li>
      <a href="02_slides_simple_linear_regression.html">Simple linear regression</a>
    </li>
    <li>
      <a href="03_slides_multiple_linear_regression.html">Multiple linear regression</a>
    </li>
    <li>
      <a href="04_slides_model_visualization.html">Ploting regression models, marginal effects</a>
    </li>
    <li>
      <a href="05_slides_model_fit.html">Model fit</a>
    </li>
    <li>
      <a href="06_slides_model_assumptions.html">Assumptions of linear models and diagnostics</a>
    </li>
    <li>
      <a href="07_slides_nonlinearity.html">Linearity</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Exercise
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="01_intro_R_excercises.html">Introduction - goals of regression analysis</a>
    </li>
    <li>
      <a href="02_simple_linear_regression_excercises.html">Simple linear regression</a>
    </li>
    <li>
      <a href="03_multiple_linear_regression_excercises.html">Multiple linear regression</a>
    </li>
    <li>
      <a href="035_multiple_linear_regression_excercises_2.html">Interactions</a>
    </li>
    <li>
      <a href="04_model_visualization_exercises.html">Ploting regression models, marginal effects</a>
    </li>
    <li>
      <a href="05_model_fit_exercises.html">Model fit</a>
    </li>
    <li class="dropdown-header">Assumptions of linear models</li>
    <li class="dropdown-header">Regression diagnostics</li>
    <li class="dropdown-header">Linearity</li>
    <li class="dropdown-header">Homoscedasticity</li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Materials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="completion_requirements.html">Syllabus</a>
    </li>
    <li>
      <a href="course_data.html">Datasets</a>
    </li>
    <li>
      <a href="literature.html">Literature</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://ksoc.ff.cuni.cz/">Department website</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Quick recap on selected concepts in statistics</h1>

</div>


<p>We encourage you to read through the following selected statistical concepts before taking the course. You can think of these concepts as informal prerequisite for taking the course. On purpose, we do not explain all the terms we use here, because you are already expected to know them on entering the course. If you are mostly familiar with what you read here, and perhaps only need to refresh on some details, chances are you are prepared to take the course. If not, consider taking some introduction to statistics first. For our students, Statistics I and Statistics II are prerequisites for this course.</p>
<div id="variable-and-its-distribution" class="section level1">
<h1>Variable and its distribution</h1>
<p>A variable is an attribute, which can take different values. E.g. height is a variable as different people can have different height. So is an opinion about something.</p>
<p>For a variable with known values, we can construct its distribution. Frequency distribution uses actual counts (it provides information such as that there are five people with height between 150 and 155 cm in a given group of 50 people). Probability distribution gives probabilities of occurrence of different possible values (e.g. the probability of having height between 150 and 155 cm is 10% etc.).</p>
<p>Below, there are some commonly used plot types which help understand distribution of a variable visually.</p>
<p><img src="00_quick_recap_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p><em>Plot A</em> is a histogram displaying frequency distribution. Histograms use bins to categorize continuous variables. As a result, they are closely related to column charts.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><em>Plot B</em> uses probability density function to display probability distribution. While we cannot directly read number of occurrences for each value of variable <em>z</em>, we can perceive the relative likelihood of each value of <em>z</em>. ^[Beware, density plot is sometimes recommended as a solution to the arbitrariness of number of bins in histogram, but it actually suffers from the same problem - the final appearance depends on a smoothing parameter, so the same data can also produce multiple differently looking density plots. However, the default tends to give a useful result.)</p>
<p><em>Plot C</em> is a boxplot. It is constructed from selected distribution statistics (the 1st, 2nd, and 3rd quartile).</p>
<p><em>Plot D</em> is called a violin plot and it is a handy combination of the boxplot with and the density plot (notice the density plot from B is simply turned by 90 degrees and mirrored on both sides of the boxplot). Violin plots enable to explicitly plot key values (the median and the interquartile range) while remaining sensitive to distributional pecularities (such as bimodality in this example).</p>
</div>
<div id="variance-and-mean" class="section level1">
<h1>Variance and mean</h1>
<p>Variability within a variable can either be expressed visually (plots above), or it can be expressed with a numeric value, typically the variance or its derivative, the standard deviation.</p>
<p>Variance of the variable <em>z</em> is the expected value of the squared deviation from its mean, we use the following formula:</p>
<p><span class="math display">\[
var(z) = E[(z_i-\mu_z)^2] = \frac{\sum (z_i - \bar{z})^2}{n}
\]</span></p>
<p>We typically work with the standard deviation (variance transformed by taking its square root) rather than the variance because standard deviation is on the original scale of the variable distribution. The formula is as follows:</p>
<p><span class="math display">\[
\sigma_z = \sqrt{var{(z)}}
\]</span></p>
<p>We also use point estimates to simplify a variable. A typical example is the arithmetic mean.</p>
<p>We refer to the arithmetic mean of variable <em>z</em> as <em>E(z)</em> generally, or μ<sub>z</sub> to refer specifically to population mean or <span class="math inline">\(\overline{x}\)</span> to refer to sample mean.</p>
</div>
<div id="sampling-distribution" class="section level1">
<h1>Sampling distribution</h1>
<p>Imagine you collect a survey sample from a population. It is only one of many and many theoretical samples you could have collected (e.g, there are many ways you can sample 1000 people from the Czech population). It follows that when you use the sample to calculate some values (mean of a variable, variance of a variable, … regression coefficient for a specified model), these are not the only possible values. In fact, each of these values is just one data point from a theoretical distribution of all the different estimates you could obtain from all the possible samples. This theoretical distribution is called sampling distribution.</p>
<blockquote>
<p>“The sampling distribution is the set of possible datasets that could have been observed if the data collection process had been re-done, …” <span class="citation">(Gelman et al., 2020, p. 50)</span></p>
</blockquote>
<p>Obviously, we usually only have one sample, hence one data point from the sampling distribution. In other words, we have no variation in our estimates, e.g., we only have one sample mean available. But we have variation in the data and that is what we use to estimate (or conceptualize) the expected variation in the estimates. (For regression coefficients, this is where assumptions are important, we will talk about it in future lectures.)</p>
<p>Standard deviation of the sampling distribution (i.e., of the distribution of an estimate) is called standard error and it is estimated as <span class="math inline">\(\frac{\sigma}{\sqrt n}\)</span>.</p>
<p>Confidence intervals are an extension of standard errors. When we can approximate the sampling distribution to normal distribution, the 95 % confidence intervals are constructed by subtracting/adding ca 2 standard errors to the point estimate, see picture below (beta-hat is the estimate of the regression coefficient beta from the data).</p>
<p><img src="images/sampling_distribution.PNG" width="50%" style="display: block; margin: auto;" /></p>
<center>
<p><font size="1">Source: <span class="citation">(Gelman et al., 2020, p. 51)</span></font></p>
</center>
<p>Note that the sampling distribution is not technically normal. It is Student’s t-distribution (or just t-distribution) which will only converge to normal when the number of observations is large enough, ca 30 and more. For smaller N, the approximation of 2 standard errors for constructing 95% confidence interval may be too much of a simplification (so much so as to be incorrect for practical applications).</p>
</div>
<div id="statistical-significance-and-hypothesis-testing" class="section level1">
<h1>Statistical significance and hypothesis testing</h1>
<p>Conventional wisdom says: statistical significance is p-value less than 0.05, relative to some null hypothesis (hypothesis of no difference). Fair enough, but remember that the 0.05 value is arbitrary.</p>
<blockquote>
<p>“[p-value is] the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” <span class="citation">(Wasserstein &amp; Lazar, 2016)</span></p>
</blockquote>
<p>Intuition behind statistical significance: an estimate is said to be NOT statistically significant if the observed value could reasonably be explained by chance.</p>
<p>This thinking arises from a still dominant yet increasingly criticized approach of so called null-hypothesis testing (NHT):</p>
<ul>
<li>null hypothesis: estimate = 0</li>
<li>alternative hypothesis: estimate != 0</li>
</ul>
<div id="critique-of-nht" class="section level2">
<h2>Critique of NHT</h2>
<ul>
<li>NH is unrealistic in social sciences where everything is linked to everything (only a matter of sample size, with enough data, almost everything gets statistically significant)</li>
<li>NH is theoretically uninteresting (effect size and variations of effect sizes in different groups is what really matters, not the unambitious claim that some difference exists)</li>
<li>NH is a very low threshold for any analysis, because non-rejection tells us that there is not even enough information in the data to move beyond the banal null hypothesis of no difference.</li>
<li>Even statistically non-significant data can carry important information (e.g., for meta-analysis or updating our priors in Bayesian approach).</li>
</ul>
<p>Despite this critique, many believe that NHT can still be useful to set some shared threshold in a discipline, especially when we tend to use similar sample sizes. But it has tobe used correctly. Often, it is not, see below.</p>
</div>
<div id="critique-of-malpractice-in-using-p-value" class="section level2">
<h2>Critique of malpractice in using p-value</h2>
<ul>
<li>p-value is often used as a license for making a claim of a scientific finding (or implied truth) while neglecting many other important considerations (“design of a study, the quality of the measurements, the external evidence for the phenomenon under study, and the validity of assumptions that underlie the data analysis.” <span class="citation">(Wasserstein &amp; Lazar, 2016)</span></li>
<li>p-value is often used incorrectly (significance chasing a.k.a. p-hacking): (1) multiple statistical tests, (2) choice of data to be presented based on statistical-significance result.</li>
<li>p-value is often interpreted incorrectly (such as when non-significant p-value is considered evidence for no difference). Please, be sure you understand why this is a problem. Feel free to ask about this in the class, but it is important that you understand.</li>
<li>There is (usually almost) no difference between 5.1% and 4.9% significance level.</li>
</ul>
</div>
</div>
<div id="examining-relationship-between-variables" class="section level1">
<h1>Examining relationship between variables</h1>
<p>In social sciences, we often want to know how two variables are associated, i.e., how they vary together (co-vary).</p>
<p>A basic measure of association is called covariance. Many measures of association draw on it one way or another. It is very closely related to variance. Inspect the the formulas below to see for yourself:</p>
<p><span class="math display">\[
var(x) = E[(x_i - \mu_x)^2] = E[(x_i - \mu_x)*(x_i - \mu_x)] = \\ \frac{\sum[(x_i - \mu_x)*(x_z-\mu_x)]}{n}
\]</span></p>
<p><br></p>
<p><span class="math display">\[
cov(x) =E[(x_i - \mu_x)*(y_i - \mu_y)] = \\ \frac{\sum[(x_i - \mu_x)*(y_i-\mu_y)]}{n}
\]</span></p>
<p>The value of covariance is rarely useful as the end product. Just like we prefer standard deviation as a scaled form of variance, we prefer correlation as scaled form of covariance. While standard deviation is scaled to the scale of the original variable, correlation is scaled to take a value between -1 and 1. In this sense, correlation is standardized covariance and measures strength of association.</p>
<p>If you square Pearson correlation coefficient between two variables, you get the proportion of variance in one variable which can be predicted by knowledge of the value for the other variable. This relationship is symmetrical (if x explains 20% of variation in y, then y explains 20% of variation in x). When you think about it, this means that we should not understand correlation coefficient as linear in the sense that increase of correlation coefficient by 0.1 always represent the same increase in the strength of association. If the coefficient is 0.2, we can explain only 0.04, i.e., 4% of variance in x by knowing the value of y. If the coefficient is 0.3, it is 9%. If the coefficient is 0.4, it is 16%. So moving from 0.2 to 0.3 means 5 percentage point decrease of unexplained variance, whereas moving from 0.3 to 0.4 means 7 percentage point decrease of unexplained variance. And so on.</p>
<p>Visual representation of different values of Pearson correlation coefficient puts things into perspective:</p>
<p><img src="00_quick_recap_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><font size="1"> Code by whuber from: <a href="https://stats.stackexchange.com/questions/15011/generate-a-random-variable-with-a-defined-correlation-to-an-existing-variables" class="uri">https://stats.stackexchange.com/questions/15011/generate-a-random-variable-with-a-defined-correlation-to-an-existing-variables</a></font></p>
<p>Correlation coefficients and regression coefficients (which will be covered in the course) are conceptually a different thing. While correlation coefficient shows the strength of association, regression coefficient shows how, on average, the value of y changes when the value of x changes by one unit (we will talk about this more). The fact that the regression lines in the set of plots above get steeper and steeper as the correlation gets stronger results from the data generation process. But it is no necessity. There can be steeper regression lines between less correlated variables and <em>vice versa</em>. See another set of simulated data below which demonstrates it.</p>
<p><img src="00_quick_recap_files/figure-html/unnamed-chunk-4-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>The four plots below show that the message sent out by visualization of a relationship between two variables can differ based on the tools we use. The plots below all visualize association between the same two continuous variables. Plotting just the regression line or just the points can send out a fairly different message about the association. Combination of the two seems more appropriate in this particular situation. Also remember that using straight line is not the only way to model association between two variables. In this case, we let the line in model D follow the data more closely (the line bends a little). However, the departure from straight line is so minor that straight line actually does seem a good approximation in this example.</p>
<p><img src="00_quick_recap_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Similarly, the message sent out by visualization can be strongly influenced by plotting decisions (or defaults) which have no relation to the data. See below two plots of the same relationship, but plotted with different width and y scale limits. Greater width makes the slope of the line seem less steep. Looser y scale limits (70 to 90 as opposed to 74 to 82 further strengthen this effect).</p>
<p><img src="00_quick_recap_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-gelman2020" class="csl-entry">
Gelman, A., Hill, J., &amp; Vehtari, A. (2020). <em>Regression and other stories</em>. Cambridge University Press. <a href="https://doi.org/10.1017/9781139161879">https://doi.org/10.1017/9781139161879</a>
</div>
<div id="ref-wasserstein2016" class="csl-entry">
Wasserstein, R. L., &amp; Lazar, N. A. (2016). The ASA statement on p-values: Context, process, and purpose. <em>The American Statistician</em>, <em>70</em>(2), 129–133. <a href="https://doi.org/10.1080/00031305.2016.1154108">https://doi.org/10.1080/00031305.2016.1154108</a>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>While not our concern here, the impression from a histogram can be <a href="https://www.researchgate.net/figure/Influence-of-the-number-of-bins-on-the-histogram-The-number-of-bins-chosen-by-the_fig7_276354826">strongly influenced by the number of bins</a>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
