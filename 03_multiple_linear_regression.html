<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Multiple linear regression</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<link href="site_libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
<link href="site_libs/_Fira Sans-0.4.0/font.css" rel="stylesheet" />
<link href="site_libs/_Fira Code-0.4.0/font.css" rel="stylesheet" />
<script src="site_libs/bs3compat-0.3.1/transition.js"></script>
<script src="site_libs/bs3compat-0.3.1/tabs.js"></script>
<script src="site_libs/bs3compat-0.3.1/bs3compat.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>




<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Applied Regression in R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lecture Notes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="00_quick_recap.html">Quick recap on selected concepts in statistics</a>
    </li>
    <li>
      <a href="01_intro.html">Introduction - goals of regression analysis</a>
    </li>
    <li>
      <a href="02_simple_linear_regression.html">Simple linear regression</a>
    </li>
    <li>
      <a href="03_multiple_linear_regression.html">Multiple linear regression</a>
    </li>
    <li>
      <a href="04_model_visualization.html">Ploting regression models, marginal effects</a>
    </li>
    <li>
      <a href="05_model_fit.html">Model fit</a>
    </li>
    <li>
      <a href="06_assumptions.html">Assumptions of linear models</a>
    </li>
    <li>
      <a href="06_diagnostics.html">Regression diagnostics</a>
    </li>
    <li>
      <a href="07_linearity_and_normality.html">Linearity</a>
    </li>
    <li>
      <a href="08_heterscedasticity.html">Homoscedasticity</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Slides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="01_slides_purpose.html">Introduction - goals of regression analysis</a>
    </li>
    <li>
      <a href="02_slides_simple_linear_regression.html">Simple linear regression</a>
    </li>
    <li>
      <a href="03_slides_multiple_linear_regression.html">Multiple linear regression</a>
    </li>
    <li>
      <a href="04_slides_model_visualization.html">Ploting regression models, marginal effects</a>
    </li>
    <li>
      <a href="05_slides_model_fit.html">Model fit</a>
    </li>
    <li>
      <a href="06_slides_model_assumptions.html">Assumptions of linear models and diagnostics</a>
    </li>
    <li>
      <a href="07_slides_nonlinearity.html">Linearity</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Exercise
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="01_intro_R_excercises.html">Introduction - goals of regression analysis</a>
    </li>
    <li>
      <a href="02_simple_linear_regression_excercises.html">Simple linear regression</a>
    </li>
    <li>
      <a href="03_multiple_linear_regression_excercises.html">Multiple linear regression</a>
    </li>
    <li>
      <a href="035_multiple_linear_regression_excercises_2.html">Interactions</a>
    </li>
    <li>
      <a href="04_model_visualization_exercises.html">Ploting regression models, marginal effects</a>
    </li>
    <li>
      <a href="05_model_fit_exercises.html">Model fit</a>
    </li>
    <li class="dropdown-header">Assumptions of linear models</li>
    <li class="dropdown-header">Regression diagnostics</li>
    <li class="dropdown-header">Linearity</li>
    <li class="dropdown-header">Homoscedasticity</li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Materials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="course_data.html">Datasets</a>
    </li>
    <li>
      <a href="literature.html">Literature</a>
    </li>
  </ul>
</li>
<li>
  <a href="completion_requirements.html">Completion requirements</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://ksoc.ff.cuni.cz/">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/Sociology-FA-CU/appliedregressioninr">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Multiple linear regression</h1>

</div>


<pre class="r"><code>library(tidyverse)
library(patchwork)
library(here)


countries &lt;- read.csv(here(&quot;data&quot;, &quot;countries.csv&quot;))</code></pre>
<p>The goals for this lecture are (1) understanding the concept of multiple linear regression with emphasis on interpretation of its coefficients, and (2) building and interpreting multiple regression models in R. We will also discuss coding categorical predictors as dummy variables and using interaction.</p>
<div id="introduction-to-multiple-linear-regression" class="section level1">
<h1>Introduction to multiple linear regression</h1>
<p>Multiple linear regression (MLR) is defined by having more than one predictor term. Remember the general formula for simple linear regression:</p>
<p><span class="math display">\[
y = \alpha + \beta*x + \epsilon_i
\]</span></p>
<p>The formula for MLR is its fairly straightforward extension. Notice that intercept in MLR is usually called <span class="math inline">\(\beta_0\)</span> rather than <span class="math inline">\(\alpha\)</span>. The other regression coefficients are numbered betas. The numbered x-es stand for predictor terms.</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1*x_1 + + \beta_2*x_2 + ... + \epsilon_i
\]</span></p>
<p>Note that having more predictor terms does not necessarily imply using more predictor variables. For example, the blue line in the following figure represents linear model estimated using three predictor terms: democratic index, democratic index to the power of two (quadratic term), and democratic index to the power of three (cubic term). While the model only uses one predictor variable, there are three predictor terms which makes the model a case of MLR. For this model, four betas would be estimated (one of them the intercept).</p>
<p><img src="03_multiple_linear_regression_files/figure-html/multiple1-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>One of the (many?) common misconceptions about linear regression is that it can only estimate a straight line. That is only true for simple linear regression. From the picture above, it should be clear that in MLR, it is possible to “bend” the line. And not only that.</p>
<p>Another example of a MLR model is visualized on the figure below. It uses two predictor terms (in this case, they are two distinct independent variables): again, the democratic index, and, in addition, a binary variable whether the country is post-soviet or not. Due to the fact that one of the independent variables is binary, it is possible to visualize the whole model in one two-dimensional figure as two lines. However, more complicated MLR models cannot be meaningfully visualized on top of the raw data like this. What we can do is visualize their results (apart from displaying them as numbers in a table). We will discuss later how it is done, but we should first make clear how the beta coefficients are interpreted in MLR.</p>
<p><img src="03_multiple_linear_regression_files/figure-html/multiple2-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="interpreting-coefficients-in-mlr" class="section level1">
<h1>Interpreting coefficients in MLR</h1>
<p>First, let’s run a model where life expectancy is explained with democratic index and percentage of people with university education in 15-64 age category (that is how the variable is defined).</p>
<pre class="r"><code>fit1 &lt;- lm(life_exp ~ dem_index + uni_prc, data = countries)
summary (fit1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = life_exp ~ dem_index + uni_prc, data = countries)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.1640 -0.9798 -0.0910  1.4256  3.1522 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   66.300      2.210  30.003  &lt; 2e-16 ***
## dem_index      1.786      0.415   4.303 0.000149 ***
## uni_prc       -1.460      6.548  -0.223 0.824979    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.949 on 32 degrees of freedom
##   (3 observations deleted due to missingness)
## Multiple R-squared:  0.5517, Adjusted R-squared:  0.5237 
## F-statistic: 19.69 on 2 and 32 DF,  p-value: 2.664e-06</code></pre>
<p>The output in the frame with white background above is the raw summary output which appears in R console as response to running the two lines of code. The beta coefficients are in the column called “Estimate.” Specifically, the coefficient for democratic index is 1.786, and the coefficient for university education is -1.460.</p>
<p>Interpreting coefficients in MLR can be surprisingly tricky as they are, “in part, contingent on the other variables in the model” <span class="citation">(Gelman et al., 2020, p. 131)</span>. This means that they will change when other variables are added to the model or dropped from the model.</p>
<p>We will cite <span class="citation">Gelman et al. (2020)</span> on how best to interpret them: “The coefficient <span class="math inline">\(\beta_k\)</span> is the average or expected difference in outcome <span class="math inline">\(y_k\)</span>, comparing two people who differ by one unit in the predictor <span class="math inline">\(x_k\)</span> while being equal in all the other predictors. This is sometimes stated in shorthand as comparing two people (or, more generally, two observational units) that differ in <span class="math inline">\(x_k\)</span> with all the other predictors held constant” (p. 131). We also call this conditional effect.</p>
<p>For the example above, we could say that if one country has democratic index higher by one point than another country, and they both have the same percentage of university educated people, the first country will have life expectancy higher by 1.8 years (i.e., rounding 1.786), on average.</p>
<p>Since the coefficient for percentage of university educated people has very broad confidence interval, we could also say that we find no evidence in the data, that two countries with the same level of democratic index should be expected to differ in life expectancy when they have different proportion of university educated people. That is if we believe the sample of countries justifies some generalization to some concrete or at least abstract population of countries. Someone could be tempted to make a purely descriptive interpretation with no ambition to make sample-to-population inference. He or she could try to say something along the lines that for our particular sample of countries and data from given years, we find that for two countries with the same level democratic index the life expectancy goes down by 1.5 years, on average, as the proportion of university educated increases from 0 to 100% (the variable is measured on scale from 0 to 1). This seems to us a meaningless statement and we would discourage from it, even if technically true. The really large standard errors should warn us against making interpretation even in purely descriptive situations. Saying that controlling for democratic index, there does not seem to be any association between life expectancy and proportion of university educated in our sample of countries seems much more sensible.<br />
Within descriptive modeling strategies, we use MLR to see the effects of individual variables net of the effects of all the other variables in the model. Within predictive modeling, we use MLR to improve our predictions over simple linear model (predictions based on just one predictor term often tend to be weak in social sciences). Within explanatory modeling, we use MLR for adjusting for background variables, hence discovering potentially spurious relationships.</p>
<div id="difference-based-vs.-change-based-interpretations" class="section level2">
<h2>Difference-based vs. change-based interpretations</h2>
<p>There two conceptually slightly different interpretations of MLR coefficients. The difference-based interpretation is well described as “how the outcome variable differs, on average, when comparing two groups of items that differ by 1 in the relevant predictor while being identical in all the other predictors” <span class="citation">(Gelman et al., 2020, p. 134)</span>. The change-based interpretation is well described by saying that “the coefficient is the expected change in y caused by adding 1 to the relevant predictor, while leaving all the other predictors in the model unchanged” <span class="citation">(Gelman et al., 2020, p. 134)</span>.</p>
<p>In others words, the difference-based interpretation uses the idea of difference between individuals, whereas the change-based interpretation uses the idea of change within individual. To be on the save side, we recommend interpreting regression coefficients as comparisons between units, not (potential) changes within units, unless one specifically claims causality. The difference-based interpretation is simply more general, hence carries less risk of being used in a misleading way.</p>
<p>Note that <span class="citation">(Gelman et al., 2020)</span> use the term “predictive interpretation” instead of difference-based interpretation and “counterfactual” interpretation instead of change-based interpretation.</p>
</div>
<div id="comparing-beta-coefficients-of-different-predictors" class="section level2">
<h2>Comparing beta coefficients of different predictors</h2>
<p>It can be sometimes useful to compare coefficients of different independent variables in the model. As we saw in the model above, it cannot be done in a straightforward way. While values of 1.786 and 1.460 are not very far from each other, it makes no sense to compare them. The first shows change when two countries differ in democratic index (measured on the scale from 0 to 10) by one (a realistic situation), whereas the latter shows change when one country has no university educated people and the other has only university educated people (a very unrealistic situation). Indeed, the coefficient value depends on the unit we use for measuring the independent variable.</p>
<p>To deal with this issues of no comparability, we can use so called standardized betas. Those can be used to determine relative weight of independent variables as they show the effect of an increase in X by one standard deviation on Y, also measured in standard deviations. The way to calculate standardized betas is to first standardize (compute z-scores) all variables used in the model and then run a regression on the z-scores.</p>
<p>A way to do this using the scale function is shown in the code below.</p>
<pre class="r"><code>fit2 &lt;- lm(scale(life_exp) ~ scale(dem_index) + scale(uni_prc), data = countries)
summary (fit2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = scale(life_exp) ~ scale(dem_index) + scale(uni_prc), 
##     data = countries)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.47433 -0.34693 -0.03223  0.50475  1.11609 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      -0.02280    0.11802  -0.193 0.848054    
## scale(dem_index)  0.82340    0.19138   4.303 0.000149 ***
## scale(uni_prc)   -0.04001    0.17945  -0.223 0.824979    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6899 on 32 degrees of freedom
##   (3 observations deleted due to missingness)
## Multiple R-squared:  0.5517, Adjusted R-squared:  0.5237 
## F-statistic: 19.69 on 2 and 32 DF,  p-value: 2.664e-06</code></pre>
<p>Now we can compare the relative strengths of our predictors. The democratic index (with standardized beta of 0.82) is a way stronger predictor of life expectancy than the proportion of university educated (0.04 in absolute value as the sign is irrelevant for this comparison). Notice that the values of t-tests for the significance of coefficients did not change. It should come as no surprise, we did not substantively change the data by standardizing it.</p>
<p><span class="citation">Gelman (2008)</span> suggests to use slightly different transformation if we also want include binary predictors (dummy variables) in the model. He shows that standardizing by subtracting the mean and dividing by 2 standard deviations (rather than just 1) enables to directly compare this kind of standardized betas with coefficients for non-standardized binaries.</p>
</div>
</div>
<div id="main-effects-and-interactions" class="section level1">
<h1>Main effects and interactions</h1>
<p>All the beta coefficients in the examples above were so called main effects. We use this term to distinguish them from interactions which we will introduce in this section. First about the main effects. There are two types of main effects. First, in simple linear regression (with one predictor term), the main effect indicates the bivariate linear association between independent and dependent variable. Second, in MLR, the main effect is the conditional effect described above (i.e., the effect of independent variable on a dependent variable conditional on all other variables in the model being held constant).</p>
<p>Using only the main effects in MLR relies on the assumption that the effect of a given independent variable on the dependent variable is the same across all levels of other independent variables in the model, or more precisely, that rather than looking at different effects of our independent variable of interest for different levels of other independent variables, it is an acceptable simplification to take the average effect.</p>
<p>This is easier explained with an example. Imagine you model the effect of years of education on earnings and you also enter gender into the model (we assume it only takes two values, “man” or “woman.” The equation would like like this: $ earnings = _0 + _1*yearsOfEducation + _2*gender$. The coefficient <span class="math inline">\(\beta_1\)</span> shows the conditional effect of years of education on income for some hypothetical average gender. This can be an acceptable simplification if the effect is similar for both men and women. But imagine there is strong positive effect of years of education on income for one gender and similarly strong negative effect for the other gender. These effects will cancel out (provided there is about the same number of men an women in the sample) and the <span class="math inline">\(\beta_1\)</span> will be close to zero. This would be a problematic simplification of the reality - we would lose a potentially important piece of information. To prevent this, we can use interactions.</p>
<p>An interaction enables to show how the effect of one independent variable on the dependent variable varies across levels of another independent variable. In the example above, the equation would look like this: <span class="math inline">\(income = \beta_0 + \beta_1*yearsOfEducation + \beta_2*gender + \beta_3*yearsOfEducation*gender\)</span>. The interpretation of coefficients is prone to errors, caution is advised. The intercept is the value of income for the reference category of gender (say man as R would code them as 0s because, alphabetically, they come before women) with 0 years of education. The coefficient for gender would show the difference for between the predicted income for men in women with no years of education. The coefficient for years of education can be thought of as the comparison of average income for men who differ by one year of education. Finally, the interaction coefficient represents the difference in slope for years of education between men and women.</p>
<p>[TBD - TABLE AND FIGURE TO ILUSTRATE THIS, EXAMPLE OF EQUATION FOR SELECTED INDIVIDUALS]</p>
<div id="model-with-interaction-vs.-two-separate-models" class="section level2">
<h2>Model with interaction VS. two separate models</h2>
<p>If we are interested in an interaction between two continuous variables, we have no other option than running a model with interaction. However, when we are interested in an interation between one continuous and one binary variable, we could also be tempted to run two separate models, one for men and one for women. This would not be a horrible solution. The estimated effected sizes would be the same. However, we would not be able to calculate the overall fit of the model (we will talk about model fit in one of the next sessions), and we would not have the estimate of standard error of the difference of slopes between men and women. So unlike in the model with interaction, we would not be able to say if the didference in slope between the two genders is statistically significant.</p>
</div>
<div id="centering-variables-in-models-with-interaction" class="section level2">
<h2>Centering variables in models with interaction</h2>
<p>We saw above that some of the interpretations of main effects can be difficult or not very useful when models contain interactions. Indeed, they are estimated with the other independent variables fixed at 0. And the 0 can be very unrealistic such as in case of years of education above.</p>
<p>However, we can use centering for better interpretability of main effects. After doing so, the main effect corresponds to a predictive difference with the other input fixed at its average value. Intercept in the example above would be the average income for a person with mean number of years of education and of “average gender.” Coefficient for years of education would be the average slope for all levels of gender. The interaction would keep the ame interpretation as above (difference of slope between genders).</p>
<p>[TBD - TABLE AND FIGURE TO ILUSTRATE THIS]</p>
</div>
<div id="interaction-as-it-depends-effects" class="section level2">
<h2>Interaction as “it depends effects”</h2>
<p>Jim Frost offers in his <a href="https://cutt.ly/HkXcMY4">blog</a> a helpful way to think about interactions as of “it depends effects.” If the answer to the question ‘What is the effect of X on Y?’ is ‘It depends,’ we are dealing with interactions. For example, do you like your food more with ketchup added? It depends, yes, if the food is french fries, no, if the food is ice-cream sundae.</p>
</div>
<div id="interactions-require-big-samples" class="section level2">
<h2>Interactions require big samples</h2>
<p>Adding interactions sometimes makes good sense theoretically, and sometimes it also helps achieve good model specification with good fit to the data. However, estimating interactions requires much bigger samples to identify the same effect size as for main effect. And since we can mostly assume that the effect size of interactions will be smaller than that of main effects, we will mostly require MUCH bigger samples. In his <a href="https://statmodeling.stat.columbia.edu/2018/03/15/need-16-times-sample-size-estimate-interaction-estimate-main-effect/">blog</a>, Andrew Gelman shows that we will need 16 times the sample size to estimate an interaction which is half the size of the main effect if we want to have the same statistical power. This does not mean that interactions are beyond reach, we can still meaningfully estimate them when their effect size is large enough. But when their effect size is only small, they can be out of reach in common sociological samples. When we look for interactions in explorative analysis, we “typically look for them is with predictors that have large coefficients when not interacted. For a familiar example, smoking is strongly associated with cancer. In epidemiological studies of other carcinogens, it is crucial to adjust for smoking both as an uninteracted predictor and as an interaction, because the strength of association between other risk factors and cancer can depend on whether the individual is a smoker.” <span class="citation">(Gelman et al., 2020, p. 136)</span>.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-gelman2008" class="csl-entry">
Gelman, A. (2008). Scaling regression inputs by dividing by two standard deviations. <em>Statistics in Medicine</em>, <em>27</em>(15), 2865–2873. https://doi.org/<a href="https://doi.org/10.1002/sim.3107">https://doi.org/10.1002/sim.3107</a>
</div>
<div id="ref-gelman2020" class="csl-entry">
Gelman, A., Hill, J., &amp; Vehtari, A. (2020). <em>Regression and other stories</em>. Cambridge University Press. <a href="https://doi.org/10.1017/9781139161879">https://doi.org/10.1017/9781139161879</a>
</div>
</div>
</div>

<br>
  <footer class="bg-white fixed-bottom border">
    <!-- Copyright -->
    <div class="text-center p-1">
      <a class="text-dark" color="black" href="https://ksoc.ff.cuni.cz/">Department of Sociology, Faculty of Arts </br> Charles University </a>
    </div>
    <!-- Copyright -->
  </footer>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
